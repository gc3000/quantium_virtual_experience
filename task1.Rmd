---
title: "Retail Strategy and Analytics"
author: Gavan Corke
output: pdf_document
---

```{r setup, include=FALSE} 
# set options for R markdown knitting 
knitr::opts_chunk$set(echo = TRUE) 
knitr::opts_chunk$set(linewidth=80)
```


```{r knitr line wrap setup, include=FALSE} 
# set up line wrapping in MD knit output 
library(knitr)
hook_output = knit_hooks$get("output")
knit_hooks$set(output = function(x, options)
{    
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth))
  {       x = knitr:::split_lines(x)
          # any lines wider than n should be wrapped        
          if (any(nchar(x) > n))             
            x = strwrap(x, width = n)        
          x = paste(x, collapse = "\n")    
   }    
   hook_output(x, options) })
```

```{r}
library(readxl)
library(data.table)
library(ggplot2)
library(ggmosaic)
library(readr)
library(stringr)
library(dplyr)
library(scales)
```



# Data Cleaning

Examine transaction data – look for inconsistencies, missing data across the data set, outliers, correctly identified category items, numeric data across all tables. 

Examine customer data – check for similar issues in the customer data.


```{r}
qvi_purchase <- read.csv("QVI_purchase_behaviour.csv")
head(qvi_purchase)

qvi_trans <- read_excel("QVI_transaction_data.xlsx")
head(qvi_trans)

# Column Names 

col_qvi <- colnames(qvi_purchase)
trans_col <- colnames(qvi_trans)
```


```{r}
#==========================#
# For transaction data set 
#==========================#

#1)

# check for data types

trans_col # names of columns 

str(qvi_trans)

dt_table<-data.frame(Names=names(qvi_trans), Type=sapply(qvi_trans,class))
labels(dt_table)

# change data DATE column into date type. 

qvi_trans$DATE <- as.Date(qvi_trans$DATE, origin = "1899-12-30")

str(qvi_trans)

#2)

#Check for missing data across the data set

sum(is.null(qvi_trans))
sum(is.na(qvi_trans))

# no null or na data. 

#3)

#Check product names are correctly entered. 


unique(qvi_trans$PROD_NAME)
summary(qvi_trans$PROD_NAME)

#Make sure we are looking at chip products. Remove any incorrect 
#products and clean the PROD NAME e.g. characters etc.\

product_unique <- unique(qvi_trans[ , "PROD_NAME"])
split_words_by <- strsplit(as.character(product_unique), " ")

productWords <- data.table(unlist(split_words_by))


setnames(productWords, 'words')

# Remove special characters from words

x <- "a1~!@#$%^&*(){}_+:\"<>?,./;'[]-="


productWords_clean <- str_replace_all(productWords, "[^[:alnum:]]", " ")

productWords_num2 <- trimws(gsub("\\w*[0-9]+\\w*\\s*", "", productWords_clean))


#productWords_final <- trimws(gsub("[^\\s]*[0-9][^\\s]*", "", productWords_clean, perl=T))

#productWords_final <- strsplit(as.character(productWords_final), " ")

# use subset to remove empty values ""

productWords_final <- subset(productWords_num2[1], productWords_num2[1]!=" ")
```

Now we can make the frequency table of the words and remove the SALSA product which is not a chip product.  

```{r}
# Frequency table
productword_freq <- strsplit(as.character(productWords_final), " ")
productword_freqtab <- table(productword_freq)

# Remove Salsa

qvi_superclean <- qvi_trans[!grepl("salsa", qvi_trans$PROD_NAME, ignore.case = TRUE), ]

```

## OUTLIERS 

In order to make sure our analysis of the products can tell us something about the general population, it is important that we investigate and deal with potential outliers as this could affect our generalization and analysis.

From a quick glance it can be seen that there is an outlier with the PROD_QTY variable. The quantity purchased from one customer totals 200 which is much higher than the average. This also affects the total sales for this transaction at being over 600. We can deal with this outlier singularly or develop a more systemic method to identify potential outliers. For example, one can generate boxplots and define the outliers to be any datapoint + or -1.5x IQR. 

Another method is to determine the relevant z-scores for each datapoint for the particular variable in question and determine if they are greater than 3 or less than -3. This will tell us that the data is 3 standard deviations below or above the mean. Both methods are outlined in this analysis however given the nature and scope of the project the more intuitive method (by glancing the 200 PROD_QTY datapoint) will be the method used here.


### Boxplot Method

```{r, eval = FALSE}
summary(qvi_superclean)

# interested in product quantity and total sales for outliers. 

qty_and_sales <- summary(qvi_superclean[c("PROD_QTY","TOT_SALES")])


# Look at boxplots for detecting outliers. 
# ======================================#

boxplot(qvi_superclean$PROD_QTY, horizontal = TRUE)
boxplot(qvi_superclean$TOT_SALES, horizontal = TRUE)

boxplot.stats(qvi_superclean$PROD_QTY)$out

hist(qvi_superclean$PROD_QTY)
hist(qvi_superclean$TOT_SALES, breaks = 150)

#Thanks to the which() function it is possible to extract the row number corresponding to these outliers:
#e.g.:  

out <- boxplot.stats(dat$hwy)$out
out_ind <- which(dat$hwy %in% c(out))
out_ind

#outliers index
# boxplot()$out shows the datapoints which are >1.5x IQR

outliers_pq <- boxplot.stats(qvi_superclean$PROD_QTY)$out
outliers_idx_pq <- which(qvi_superclean$PROD_QTY %in% c(outliers_pq))


length(qvi_superclean$PROD_QTY)

no_outliers_trans <- qvi_trans[-outliers_idx_pq, ]

#outliers sales

qvi_pq <- qvi_trans$TOT_SALES

outliers_sales <- boxplot.stats(qvi_pq)$out
outliers_idx_sal <- which(qvi_pq %in% c(outliers_idx_pq))


no_outliers_qvi <- no_outliers_trans[-outliers_idx_sal, ]

clean_qvi <- no_outliers_qvi

#IQR takes too many datapoints away
```


However for this dataset it seems to take away too many data points and thus will not be used further in this analysis. 


### Z score method. 

```{r}
#===================#

# Z score approach

# ==================#
# use z scores to get rid of outliers for PROD_QTY

z_score = (qvi_superclean$PROD_QTY - mean(qvi_superclean$PROD_QTY))/ 
  sd(qvi_superclean$PROD_QTY)
outliers_idx_pq <- which(!(-3 < z_score & z_score < 3))

#use z scores to get rid of outliers for TOT_SALES

z_score2 <- (qvi_superclean$TOT_SALES - mean(qvi_superclean$TOT_SALES))/ 
  sd(qvi_superclean$TOT_SALES)
outliers_idx_sales <- which(!(-3 < z_score2 & z_score2 < 3))


clean_qvi_trans <- qvi_superclean[-outliers_idx_sales, ]
clean_qvi_trans <- clean_qvi_trans[-outliers_idx_pq, ]
clean_qvi_trans <- clean_qvi_trans

hist(clean_qvi_trans$TOT_SALES, xlab = "Total Sales", ylab = "Frqeuency",
     main = "Total Sales per Transaction", col="dodgerblue3", density=25,
     angle=60)
hist(clean_qvi_trans$PROD_QTY, xlab = "Product Quantity Bought",
     ylab = "Frequency", main = "Total Product Quantity Bought", col="dodgerblue3",
     density=25,
     angle=60)
```

### Looking at the singular outlier. 

```{r}
# CHECK the 200 OUTLIER

qvi_superclean[qvi_superclean$PROD_QTY == 200, ]

unique(qvi_superclean[qvi_superclean$PROD_QTY == 200, ]$LYLTY_CARD_NBR)

# 226000 Loyalty Card Number
# other transactions he made

qvi_superclean[qvi_superclean$LYLTY_CARD_NBR == 226000, ]

# Remove from further analysis 

qvi_superclean2 <- qvi_superclean[qvi_superclean$LYLTY_CARD_NBR != 226000, ]

# Number of transactions by date

length(unique(qvi_superclean2$DATE))
```

```{r , eval = FALSE}
table(qvi_superclean2$DATE)
```

```{r}
#create a sequence of dates and join with the counts of transactions per date

seq_1 <- seq(from = as.Date("2018-7-1"), to = as.Date("2019-06-30"),
    by = "days")

date_trans <- as.data.frame(seq_1)

date_wit_trans <- as.data.frame(t(table(qvi_superclean2$DATE)))[ , c(2, 3)]

date_wit_trans$Var2 <- as.Date(date_wit_trans$Var2)

freq_table <- merge(date_trans, date_wit_trans, by.x = "seq_1", by.y = "Var2", all.x = TRUE)


which(is.na(freq_table$Freq))# 178

freq_table2 <- freq_table[-178, ]
freq_table2[178, ] <- freq_table[178, ]


freq_table2$Freq <- as.numeric(freq_table2$Freq)
freq_table2$seq_1 <- as.Date(freq_table2$seq_1)
```

Generating relevant graphs. 

```{r}
#Graph counts
#### Setting plot themes to format graphs
theme_set(theme_bw())
theme_update(plot.title = element_text(hjust = 0.5))

#### Plot transactions over time
ggplot(freq_table2, aes(x = seq_1, y = Freq)) +
  geom_line() +
  labs(x = "Day", y = "Number of transactions", title = "Transactions over time") +
  scale_x_date(breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

##Zoom into december

ggplot(freq_table2, aes(x = seq_1, y = Freq)) +
  geom_line() +
  labs(x = "Day", y = "Number of transactions", 
       title = "Transactions over time") +
  scale_x_date(limits = c(as.Date("2018-12-01"), as.Date("2018-12-31")))
```

From the graphs above we can see there is an uptick of sales all the way until the 25th of december. Here we can see that no transactions occur. This is the 25th of December, which is christmas day - a public holiday. 

Now that we are satisfied that the data no longer has outliers, we can move on to creating other features such as brand of chips or pack size from PROD_NAME. We will start with pack size.

## Pack Size 

```{r}
#### Pack size
#### We can work this out by taking the digits that are in PROD_NAME

qvi_superclean2$PACK_SIZE <- parse_number(qvi_superclean2$PROD_NAME)

#See the sizes in order

qvi_superclean2[order(qvi_superclean2$PACK_SIZE), "PACK_SIZE"]

summary(qvi_superclean2[order(qvi_superclean2$PACK_SIZE), "PACK_SIZE"])

#### Let's plot a histogram of PACK_SIZE since we know that it is a categorical
#variable and not a continuous variable even though it is numeric.


hist(qvi_superclean2$PACK_SIZE, xlab = "Packet Size (g)", ylab = "Frequency",
     main = "Packet Size (g) Distribution")

# FIRST WORD OF EACH Product. 
first_word <- strsplit(qvi_superclean2$PROD_NAME, " ")

brand <- list()

#for( i in 1:length(first_word)){
# brand <- c(brand, first_word[[i]][1])
# }

#OR

first_words <- sapply(first_word, function(x) strsplit(x, " ")[[1]][1])

qvi_superclean2$brands <- first_words

# make sure the same products are referred to by the same string

qvi_superclean3 <- qvi_superclean2


qvi_superclean3$brands <- gsub("RRD", "Red", qvi_superclean3$brands )
qvi_superclean3$brands <-gsub("GrnWves", "Grain", qvi_superclean3$brands )
qvi_superclean3$brands <-gsub("Snbts", "Sunbites", qvi_superclean3$brands )
qvi_superclean3$brands <-gsub("Infzns", "Infuzions", qvi_superclean3$brands )
qvi_superclean3$brands <-gsub("Smiths", "Smith", qvi_superclean3$brands )
qvi_superclean3$brands <-gsub("WW", "Woolworths", qvi_superclean3$brands )

unique(qvi_superclean3$brands)
```

From the summary of the pack sizes we find that the minimum size is 70g, the median size is 170g and the maximum size (the largest chip packet size) is 380g. In Australian supermarkets you can find 70g packet sizes and the 380g size is usually called a 'party size' as it is to be shared among a few people. Thus the sizes shown in the summary seem fine. There is no need to worry about any potential outliers or high leverage points with regards to packet size. 

## The Purchase Dataset 

```{r}
#==========================#
# For purchase data set 
#==========================#

head(qvi_purchase)

#1)

# check for data types

trans_col # names of columns 

str(qvi_purchase)

dp_table<-data.frame(Names=names(qvi_purchase), Type=sapply(qvi_purchase,class))
labels(dp_table)

#2)
# check for null or na values

sum(is.na(qvi_purchase))
sum(is.null(qvi_purchase))

#3)

#Check product names are correctly entered. 


unique(qvi_purchase$LIFESTAGE)
unique(qvi_purchase$PREMIUM_CUSTOMER)

ggplot(qvi_purchase, aes(x = LIFESTAGE, fill = LIFESTAGE)) +
     geom_histogram(stat = "count")+
  theme(axis.text.x = element_text(angle=45, hjust=1))+
  ggtitle("Customer Distribution by Lifestage")


ggplot(qvi_purchase, aes(x=PREMIUM_CUSTOMER, fill = PREMIUM_CUSTOMER))+
         geom_histogram(stat = "count")+
  ggtitle("Customer Distribution by Consumer Behaviour")


# merge dataframes 

qvi_data <- merge(qvi_superclean3, qvi_purchase, by = 'LYLTY_CARD_NBR')
dim(qvi_data)

# I think these two (above) and below are the same left joins. 

qvi_data2 <- merge(qvi_superclean3, qvi_purchase, all.x = TRUE)

#Let's also check if some customers were not matched on by checking for nulls.

is.null(qvi_data2$LYLTY_CARD_NBR) # No Nulls. 

fwrite(qvi_data2, "qvi_data2.csv", sep = ",", row.names = FALSE)
```

# DATA ANALYSIS 

In this section we can perform data analysis on the merged data set to gleam any particular insights. This can be done via filtering the data via particular sub categories and looking at variables of interest to ideally discover some underlying trends that can be help in improving product sales in the future. 



```{r}
# ToTAL SALES BY BRAND 

agg_brand_sales <- aggregate(qvi_data2$TOT_SALES, by = list(qvi_data2$brands), 
                             sum)

ggplot(agg_brand_sales, aes(x = reorder(Group.1, x), y = x, fill = Group.1)) +
  geom_bar(position = 'dodge', stat = 'identity', show.legend = FALSE) +
  labs(x = "Chips Brand", y= "Total Sales", title = "Total Sales by Brand")+
  scale_y_continuous(labels = comma)+
  coord_flip()
```


Here we can see that Kettle's chips are by far the brand with the most sales. Coming in second and third are Smith's and Dorito's respectively. 

One metric we can look at is who spends the most on chips by life stage and general purchasing behaviour. 

```{r}
sales_by_life <- aggregate(qvi_data2$TOT_SALES, 
                           by = list(qvi_data2$LIFESTAGE), sum)

sales_by_customer <-aggregate(qvi_data$TOT_SALES, 
                              by = list(qvi_data2$PREMIUM_CUSTOMER), sum)



rotate_x <- function(data, column_to_plot, labels_vec, rot_angle, title) {
    plt <- barplot(data[[column_to_plot]], col='dodgerblue3', 
                   xaxt="n", density = 25,
                   angle = 60, main = title)
    text(plt, par("usr")[3], labels = labels_vec, srt = rot_angle, 
         adj = c(1.1,1.1), xpd = TRUE, cex=0.6) 
}

rotate_x(sales_by_life, 'x', sales_by_life$Group.1, 45, "Total Sales by Life Stage")


barplot(sales_by_customer$x, beside = TRUE, names.arg=sales_by_customer$Group.1,
        main = "Total Sales by General Purchasing Behaviour",
        col ='dodgerblue3', angle = 60, density = 25)
```

In the first graph above we can see that order singles and couples contribute to the largest amount of sales compared to other groups filtered just by life stage. In the second graph we can see that the mainstream group contribute to the highest amount of total sales compared to all other groups. 

```{r}
# Find total sales by LIFESTAGE and CUSTOMER BEHAVIOUR, then divide each by PROD_QTY for each category

total_product_qty_life <- aggregate(qvi_data2$PROD_QTY, 
                                    by = list(qvi_data2$LIFESTAGE), sum) 

avgprice_qty_lifestage <- sales_by_life[,2]/total_product_qty_life[,2]

avgprice_qty_lifestage

barplot(sort(avgprice_qty_lifestage, decreasing = TRUE), beside = TRUE, 
        names.arg=total_product_qty_life$Group.1,
        main = "Average Chip Price by Life Stage", las = 2, col = 'dodgerblue3',
        angle = 60, density = 25, cex.names = 0.7)

summary(avgprice_qty_lifestage)

total_product_qty_customer_behaviour <- aggregate(qvi_data2$PROD_QTY, by = list(qvi_data2$PREMIUM_CUSTOMER), sum) 

avgprice_qty_customer_behaviour <- sales_by_customer[,2]/total_product_qty_customer_behaviour[,2]

avgprice_qty_customer_behaviour

barplot(avgprice_qty_customer_behaviour, beside = TRUE, 
        names.arg = total_product_qty_customer_behaviour$Group.1,
        main = "Average Chip Price by Customer Behaviour")
```



```{r}
#### Total sales by LIFESTAGE and PREMIUM_CUSTOMER


tot_sales_premium_life <- aggregate(TOT_SALES~LIFESTAGE+PREMIUM_CUSTOMER, 
                                    qvi_data2, sum)

ggplot(tot_sales_premium_life, aes(x = PREMIUM_CUSTOMER, 
                                   y = TOT_SALES,fill = LIFESTAGE)) +
  geom_bar(position = 'dodge', stat = 'identity')+
  labs (x = "Customer Type", y = "Total Sales", title = "Total Sales")
```

In this graph we can see that older families contribute the most to total budget sales, whilst for the mainstream, young couples and singles contribute the highest percentage of sales. Lastly, in the premium category, older singles and couples are the highest contributers to total sales. 

```{r}
#Number of Customers by LIFESTAGE and PREMIUM_CUSTOMER


filtered_lylty_card <- qvi_data2[unique(qvi_data2$LYLTY_CARD_NBR), ]

number_customers_premium_life <- aggregate(LYLTY_CARD_NBR~LIFESTAGE+PREMIUM_CUSTOMER,
                                       filtered_lylty_card, length)

ggplot(number_customers_premium_life, aes(x =PREMIUM_CUSTOMER,
                                          y = LYLTY_CARD_NBR,fill = LIFESTAGE))+
  geom_bar(position = 'dodge', stat = 'identity')+
  labs(x = "Customer Type", y = "Customers", title = "Number of Customers")
```




```{r}
# Average number of units per customer by LIFESTAGE and PREMIUM_CUSTOMER

units_per_category <- aggregate(PROD_QTY~LIFESTAGE+PREMIUM_CUSTOMER,
                                qvi_data2, sum)

units_per_customer_category <- units_per_category

units_per_customer_category[, 4] <- units_per_category[,3]/number_customers_premium_life[,3]

ggplot(units_per_customer_category, aes(x =PREMIUM_CUSTOMER, 
                                        y = V4,fill = LIFESTAGE)) +
  geom_bar(position = 'dodge', stat = 'identity')+
  labs(x = "Customer Type", y = "Avg Number of Units", 
       title = "Average Number of Units Purchased")

#Average price per unit in LIFESTAGE and PREMIUM_CUSTOMER

average_price_per_unit <- units_per_category 

average_price_per_unit[, 4] <- tot_sales_premium_life[ , 3]

average_price_per_unit$total_sales <- average_price_per_unit[ ,4] 

average_price_per_unit <- average_price_per_unit[, -4]

average_price_per_unit_LIFE_PREMIUM <- average_price_per_unit

average_price_per_unit_LIFE_PREMIUM[ , 5] <- average_price_per_unit[,4]/
  average_price_per_unit[3]

names(average_price_per_unit_LIFE_PREMIUM)[5] <- c("Average_Price")

ggplot(average_price_per_unit, aes(x =PREMIUM_CUSTOMER, 
                                   y =total_sales/PROD_QTY,fill = LIFESTAGE)) +
  geom_bar(position = 'dodge', stat = 'identity')+
  labs(x = "Customer Type", y = "Total Sales", title = "Average Price per Unit")
```

Mainstream midage and young singles and couples are more willing to pay more per
packet of chips compared to their budget and premium counterparts. This may be due
to premium shoppers being more likely to buy healthy snacks and when they buy
chips, this is mainly for entertainment purposes rather than their own consumption.
This is also supported by there being fewer premium midage and young singles and
couples buying chips compared to their mainstream counterparts.


# T-Test

In this section, we will perform a T-test between mainstream vs premium and budget mid age and young singles and couples. Therefore in order to do this we must state our null and alternative hypothesis:

H_{0} = The null hypothesis states that there is NO significant difference between the two groups.

H_{1} = The alternate hypothesis states that there is a significant difference between the two groups (mainstream mid age and young singles and couples vs premium and budget mid age and young singles and couples).

In order to reject the null hypothesis we will use the standard p-value of being less than or equal to 0.05.

```{r}

#T test is between two groups mainstream midage and young singles/couples vs
# premimum and mainstream midage and young singles/couples


# null hypothesis, there is not a statistically significant difference between 
# the two groups.

df_main <- qvi_data2 %>% filter(PREMIUM_CUSTOMER == "Mainstream", 
                                LIFESTAGE %in% c("MIDAGE SINGLES/COUPLES",
                                                 "YOUNG SINGLES/COUPLES"))

df_other <- qvi_data2 %>% filter(PREMIUM_CUSTOMER %in% c("Budget", "Premium"),
                                 LIFESTAGE %in% c("MIDAGE SINGLES/COUPLES",
                                                  "YOUNG SINGLES/COUPLES"))
main_avgsales <- df_main$TOT_SALES/df_main$PROD_QTY

other_avgsales <- df_other$TOT_SALES/df_other$PROD_QTY

tTest <- t.test(x = main_avgsales, y = other_avgsales, 
                alternative = "two.sided", var.equal = TRUE)

tTest
```
# RESULTS


The t-test results in a p-value of <2.2e-16, i.e. the unit price for mainstream, young and mid-age singles and couples ARE significantly higher than that of budget or premium, young and mid age singles and couples. Therefore we can reject the null hypothesis that there is no significant difference between the two groups. 

This result will enable us to develop strategies and recommendations which could improve sales as we now know that there is a quantitative difference suggested by the data among the groups of concern. Thus we can optimize and focus our strategies to each respective group in a more tailored and meaningful way.  
